\name{cleanSizeFactors}
\alias{cleanSizeFactors}

\title{Sanitize size factors}
\description{Coerce non-positive size factors to positive values based on the number of detected features.}

\usage{
cleanSizeFactors(size.factors, num.detected, 
    control=nls.control(warnOnly=TRUE),
    iterations=3, nmads=3, ...) 
}

\arguments{
\item{size.factors}{A numeric vector containing size factors for all libraries.}
\item{num.detected}{A numeric vector of the same length as \code{size.factors}, containing the number of features detected in each library.}
\item{control}{Argument passed to \code{\link{nls}} to control the fitting, see \code{?\link{nls.control}} for details.}
\item{iterations}{Integer scalar specifying the number of robustness iterations.}
\item{nmads}{Numeric scalar specifying the multiple of MADs to use for the tricube bandwidth in robustness iterations.}
\item{...}{Further arguments to pass to \code{\link{nls}}.}
}

\details{
This function will first fit a non-linear curve of the form
\deqn{y = \frac{ax}{1 + bx}}{y = ax/(1 + bx)}
where \code{y} is \code{num.detected} and \code{x} is \code{size.factors} for all positive size factors.
This is a purely empirical expression, chosen because it is passes through the origin, is linear near zero and asymptotes at large \code{x}.
The fitting is done robustly with iterations of tricube weighting to eliminate outliers.

We then consider the number of detected features for all samples with non-positive size factors.
This is treated as \code{y} and used to solve for \code{x} based on the curve fitted above.
The result is the \dQuote{cleaned} size factor, which must always be positive for \code{y < a/b}.
For \code{y > a/b}, there is no solution so the cleaned size factor is defined as the largest positive value in \code{size.factors}.

Negative size factors can occasionally be generated by \code{\link{computeSumFactors}}, see the documentation there for more details.
By coercing them to positive values, we can proceed to normalization and downstream analyses.
Here, we use the number of detected features as this is more robust to differential expression that would cause biases in the library size.
Of course, it is not theoretically guaranteed to yield the correct size factor, but a rough guess is better than a negative value.
}

\author{
Aaron Lun
}

\seealso{
\code{\link{nls}},
\code{\link{computeSumFactors}}
}

\examples{
set.seed(100)    
counts <- matrix(rpois(20000, lambda=1), ncol=100)

# Adding negative values:
cleanSizeFactors(c(-1, colSums(counts)), c(100, colSums(counts>0)))
}
